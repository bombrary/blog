<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>誤差逆伝播法 on Chanomic Blog</title>
    <link>https://bombrary.github.io/blog/tags/%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95/</link>
    <description>Recent content in 誤差逆伝播法 on Chanomic Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sat, 15 Jan 2022 18:45:40 +0900</lastBuildDate>
    <atom:link href="https://bombrary.github.io/blog/tags/%E8%AA%A4%E5%B7%AE%E9%80%86%E4%BC%9D%E6%92%AD%E6%B3%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ニューラルネットワーク ノート - 誤差逆伝播の計算</title>
      <link>https://bombrary.github.io/blog/posts/nn-backprop/</link>
      <pubDate>Sat, 15 Jan 2022 18:45:40 +0900</pubDate>
      <guid>https://bombrary.github.io/blog/posts/nn-backprop/</guid>
      <description>誤差逆伝播法の数式の説明なんて世の中にたくさんあると思うが、理解のために自分でもまとめる。 特に添字などのミスがあると思うので、見つけ次第修正する。&#xA;誤差逆伝播の計算 (1) 問題設定 入力を第 $0$ 層、出力を第 $L$ 層とする。ニューラルネットワークはよく次のようなグラフで描かれる。 円がノードを表す。ノードに入っていく矢印が入力、出ていく矢印が出力を表す。&#xA;第 $l$ 層 $j$ 番目のノードの出力を $y_j^l$ とおく (注意: この記事では $y_j^{l}$ の $l$ は添字を表すものとする。累乗ではない。これから現れる変数についても同様)。これはある関数 $f_l$ を用いて以下の式で表される。$f_l$ は活性化関数と呼ばれる。 ただし、$u_j^{l}$ は前の層の出力を用いて計算される線形和で、以下のように定義される。 このような、線形和を取って $f$ を適用するという流れは次のようなグラフで描かれる。&#xA;この $\sum | f$ のノードがたくさん集まって第 $l$ 層を形成している。&#xA;損失関数 $E$ は、重み$w_{ij}^{l}$ についての関数である。これは出力値 $y_i^{L}$ と教師データ $\tilde{y}_i$ との違いを測る尺度であるから、$y_i^{L}$ の関数でもある。 例えば、以下の二乗誤差は損失関数の一種である。 定義中に $w_{ij}^{l}$ が含まれていないじゃないか、と思うかもしれないが、$y_i^{L}$ の定義中に $w_{ij}^{L}$ が含まれている。さらにその中の $y_i^{L-1}$ 中に $w_{ij}^{L-1}$ が含まれている。以下同様にして $w_{ij}^l$ は $E$ の中に含まれている。&#xA;いま、$E$ を最小化するような $w_{ij}^{l}$ を求めたい。これには確率的勾配法が利用できるが、そのために偏微分 $\displaystyle \frac{\partial E}{\partial w_{ij}^{l}}$ を計算する必要がある。以降、これをどう計算するかという話を展開していく。</description>
    </item>
  </channel>
</rss>

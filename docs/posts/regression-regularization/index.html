<!DOCTYPE html>
<html lang="ja-jp">
<title>線形回帰メモ 正則化 | Chanomic Blog</title>
<meta charset="utf-8">
<meta name="generator" content="Hugo 0.91.2" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="https://bombrary.github.io/blog/css/index.css">
<link rel="canonical" href="https://bombrary.github.io/blog/posts/regression-regularization/">
<link rel="alternate" type="application/rss+xml" href="" title="Chanomic Blog">
<link rel="stylesheet" href="https://bombrary.github.io/blog/katex/katex.min.css">
<script defer src="https://bombrary.github.io/blog/katex/katex.min.js"></script>
<script defer src="https://bombrary.github.io/blog/katex/contrib/auto-render.min.js" onload="renderMathInElement(document.body)"></script>
  <link rel="stylesheet" href="/blog/css/custom.css">

  <link rel="stylesheet" href="/blog/css/syntax.css">



<script async src="https://www.googletagmanager.com/gtag/js?id=UA-152083322-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-152083322-1');
</script>


<link rel="stylesheet" href="https://bombrary.github.io/blog/css/math.css">
<script>
  document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {delimiters: [
      {left: "$$", right: "$$", display: true},
      {left: "$", right: "$", display: false}]
    });

    
    var tex = document.getElementsByClassName("tex");
    Array.prototype.forEach.call(tex, function(el) {
      katex.render(el.getAttribute("data-expr"), el);
    });
  });
</script>


<header>
  
    <a href="https://bombrary.github.io/blog/" class="title">Chanomic Blog</a>
  
  
</header>


<article>
  <header>
    <h1><a href="https://bombrary.github.io/blog/posts/regression-regularization/">線形回帰メモ 正則化</a></h1>
    <div class="meta">
      
      <div class="pub-date">
        <time datetime="2021-08-07T07:17:00&#43;09:00">August 07, 2021</time>
      </div>
      
      
      <div class="lastmod-date">
        <div class="lastmod-date__label">(last modified:</div>
        <time datetime="2021-08-07T07:17:00&#43;09:00">August 07, 2021</time>
        <div class="lastmod-date__label">)</div>
      </div>
      
    </div>
    
    <div class="tags-categories">
      <div class="tags">
        <p>tags: </p>
        <ul class="tags_list">
          <li class="tags_item"><a href="https://bombrary.github.io/blog//tags/%e7%b7%9a%e5%bd%a2%e5%9b%9e%e5%b8%b0/">線形回帰</a></li><li class="tags_item"><a href="https://bombrary.github.io/blog//tags/%e6%ad%a3%e5%89%87%e5%8c%96/">正則化</a></li>
        </ul>
        
      </div>
      <div class="categories">
        <p>categories: </p>
        
        <ul class="categories_list">
           <li class="categories_item"><a href="https://bombrary.github.io/blog//categories/%e6%a9%9f%e6%a2%b0%e5%ad%a6%e7%bf%92/">機械学習</a></li> <li class="categories_item"><a href="https://bombrary.github.io/blog//categories/julia/">Julia</a></li>
        </ul>
        
      </div>
    </div>
  </header>
  
    <aside>
      <h2>目次</h2>
      <nav id="TableOfContents">
  <ul>
    <li><a href="#問題設定">問題設定</a></li>
    <li><a href="#正則化前のコスト関数">(正則化前の)コスト関数</a></li>
    <li><a href="#l1正則化とl2正則化">L1正則化とL2正則化</a></li>
    <li><a href="#ridge回帰">Ridge回帰</a>
      <ul>
        <li><a href="#勾配">勾配</a></li>
      </ul>
    </li>
    <li><a href="#lasso回帰">Lasso回帰</a>
      <ul>
        <li><a href="#勾配-1">勾配</a></li>
        <li><a href="#1変数以外は固定した場合の最小値">1変数以外は固定した場合の最小値</a></li>
        <li><a href="#座標降下法">座標降下法</a></li>
      </ul>
    </li>
    <li><a href="#juliaによる実装">Juliaによる実装</a>
      <ul>
        <li><a href="#データの生成">データの生成</a></li>
        <li><a href="#ridge回帰-1">Ridge回帰</a></li>
        <li><a href="#lasso回帰-1">Lasso回帰</a></li>
        <li><a href="#グラフのプロット">グラフのプロット</a></li>
        <li><a href="#main関数">main関数</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </aside>
    
  <h2 id="問題設定">問題設定</h2>
<p>$\bm{y} = (y^{(1)}, y^{(2)}, \ldots, y^{(N)})^T,\ \bm{x}_i = (1, x_1^{(i)}, x_2^{(i)}, \ldots, x_D^{(i)})^T$
とおく。$(\bm{x}_i, y_i),\ i = 1, 2, \ldots, N$ がデータとして与えられている。このとき、入力と出力の間に</p>
<p>$$
\begin{aligned}
y
&amp;= h_{\bm{w}}(\bm{x})\\
&amp;:= w_0 + w_1x_1 + w_2x_2 + \cdots + w_Dx_D\\
&amp;= \bm{w}^T\bm{x}
\end{aligned}
$$</p>
<p>が成り立つと仮定し、これに適する$\bm{w}$を見つけたい。</p>
<h2 id="正則化前のコスト関数">(正則化前の)コスト関数</h2>
<p>ここで「適する」とは具体的に何なのかというと、ここでは予測とデータとの二乗誤差の和</p>
<p>$$
J(\bm{w}) = \frac{1}{2} \sum_{i=1}^{N} (h_{\bm{w}}(\bm{x}_i) - y^{(i)})^2
$$</p>
<p>が最小となる $\bm{w}$ を求める。この $J$ をここではコスト関数と呼ぶ。
係数 $1/2$ は微分した時に出てくる $2$ を消し去るための便宜的なものであり、つける必然はない。</p>
<h2 id="l1正則化とl2正則化">L1正則化とL2正則化</h2>
<p>コスト関数に $\bm{w}_i$ のL1ノルム(の1乗)の項を付けることをL1正則化という。</p>
<p>$$
J_1(\bm{w}) = J(\bm{w}) + \lambda \|\bm{w}\|_1
$$</p>
<p>ただし、$\lambda$ は適当な定数。</p>
<p>同様に，コスト関数に $\bm{w}_i$ のL2ノルムの2乗の項を付けることをL2正則化という。</p>
<p>$$
J_2(\bm{w}) = J(\bm{w}) + \frac{\lambda}{2} \|\bm{w}\|_2^2
$$</p>
<p>ただし、$\lambda$ は適当な定数。$\lambda$ に $1/2$ をつける理由は微分し易くするためのもの。</p>
<p>一般に，コスト関数に $\bm{w}_i$ のLpノルムのp乗の項を付けることをLp正則化という。
正則化はデータの過学習を防ぐ目的で使われる。</p>
<h2 id="ridge回帰">Ridge回帰</h2>
<p>$J(\bm{w})$ にL2正則化を施したものは以下の通り。</p>
<p>$$
J_2(\bm{w}) = \frac{1}{2} \sum_{i=1}^{N} (h_{\bm{w}}(\bm{x}_i) - y^{(i)})^2 + \frac{\lambda}{2} \|\bm{w}\|_2^2
$$</p>
<h3 id="勾配">勾配</h3>
<p>L2正則化の場合は単純に微分できる。まず $w_j$ で微分すると、$\| \bm{w} \|_2^2 = \sum_{j=1}^{D}w_j^2$ であることに注意して、</p>
<p>$$
\frac{\partial J_2}{\partial w_j}
= \sum_{i=1}^{N} (h_{\bm{w}}(\bm{x}<em>i) - y^{(i)})x</em>{ij} + \lambda w_j
$$</p>
<p>となる。 これより勾配が計算できる。
$J$ の部分のベクトル表現については、<a href="https://bombrary.github.io/blog/posts/regression-gradient-descent/#勾配の計算">線形回帰の勾配法のときの計算</a>と同様にして、</p>
<p>$$
\frac{\partial J_2}{\partial \bm{w}} = X^T(X\bm{w} - \bm{y}) + \lambda \bm{w}
$$</p>
<p>ただし、</p>
<p>$$
X = \begin{pmatrix}
x_{10} &amp; x_{11} &amp; \cdots &amp; x_{1D}\\
x_{20} &amp; x_{21} &amp; \cdots &amp; x_{2D}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
x_{N0} &amp; x_{N1} &amp; \cdots &amp; x_{ND}
\end{pmatrix}
$$</p>
<h2 id="lasso回帰">Lasso回帰</h2>
<p>L1正則化</p>
<p>$J(\bm{w})$ にL2正則化を施したものを改めて $J(\bm{w})$ とおく。</p>
<p>$$
J_1(\bm{w}) = \frac{1}{2} \sum_{i=1}^{N} (h_{\bm{w}}(\bm{x}_i) - y^{(i)})^2 + \lambda \|\bm{w}\|_1
$$</p>
<h3 id="勾配-1">勾配</h3>
<p>$w_j$ で微分すると、$\| \bm{w} \|_1 = \sum_{j=1}^{D} |w_j|$ であることに注意して、</p>
<p>$$
\frac{\partial J_1}{\partial w_j}
= \sum_{i=1}^{N} (h_{\bm{w}}(\bm{x}<em>i) - y^{(i)})x</em>{ij} + \lambda \frac{\partial |w_j|}{\partial w_j}
$$</p>
<p>さて、最後の項が計算できないため、通常の勾配降下法が利用できない。いくつか方法があるらしいが、ここでは座標降下法で解くことを目指す。その準備として、「1変数のみ動かしたときの$J$の最小値」を求めることにする。</p>
<h3 id="1変数以外は固定した場合の最小値">1変数以外は固定した場合の最小値</h3>
<p>$J_1(\bm{w})$ について、 $w_d$ だけを動かすことを考え、$J_1$ が最小になる $w_d$ を求める。 &ldquo;$J_1$ の $w_d$ に関する勾配 = 0&rdquo; を解くことを目指す。</p>
<p>見やすさのため、以下の記号を定義する。</p>
<ul>
<li>$\bm{w}_{-d}$ : ベクトル $\bm{w}$ から第 $d$ 成分を取り除いたベクトル。</li>
<li>$\bm{x}_{n, -d}$ : ベクトル $\bm{x}_n$ から第 $d$ 成分を取り除いたベクトル。</li>
<li>$\bm{x}_{:, d} = (x_{1d}, x_{2d}, \ldots, x_{Nd})^T$: 行列 $X$ の第 $d$ 列を取り出したベクトル。</li>
<li>$X_{:, -d}$ : 行列 $X$ から第 $d$ 列を取り除いた行列。</li>
<li>$\bm{r}_{-d} = \bm{y} - X_{:, -d}\bm{w}_{-d}$ : データの $d$ 番目の特徴を考慮しない場合の誤差</li>
</ul>
<p>$$
\begin{aligned}
\frac{\partial J_1}{\partial w_d}
&amp;= \sum_{n=1}^{N} (\bm{w}^T\bm{x}<em>n - y^{(n)})x</em>{nd} + \lambda \frac{\partial |w_d|}{\partial w_d}\\
&amp;= \sum_{n=1}^{N} (\bm{w}<em>{-d}^T\bm{x}</em>{n,-d} + w_dx_{nd} - y^{(n)})x_{nd} + \lambda \frac{\partial |w_d|}{\partial w_d}\\
&amp;= \sum_{n=1}^{N} (\bm{w}<em>{-d}^T\bm{x}</em>{n,-d} - y^{(n)})x_{nd} + w_d\sum_{n=1}^{N} x_{nd}^2 + \lambda \frac{\partial |w_d|}{\partial w_d}\\
&amp;= -x_{:, d}^T \bm{r}_{-d} + w_d\| \bm{x}_{:, d} ||_2^2 + \lambda \frac{\partial |w_d|}{\partial w_d}\\
&amp;= -c_d + w_da_d + \lambda \frac{\partial |w_d|}{\partial w_d}
\end{aligned}
$$</p>
<p>ここで、再び見やすさのため、$c_d = x_{:, d}^T \bm{r}_{-d},\ a_d = \| \bm{x}_{:, d} ||_2^2$とおいた。</p>
<p>さて、&ldquo;勾配 = 0&rdquo; を解いて局所最小値を求めたいが、最後の項が $w_d = 0$ で微分不可能。
そこで、劣勾配の概念を利用する。$|w_d|$ の劣勾配は以下の通り。</p>
<p>$$
\frac{\partial |w_d|}{\partial w_d} =
\begin{cases}
{ -1 } &amp; (w_d &lt; 0)\\
[ -1, 1] &amp; (w_d = 0)\\
{ 1 } &amp; (w_d &gt; 0)
\end{cases}
$$</p>
<p>$J_1$ の劣勾配は、勾配と区別せず $\frac{\partial J_1}{\partial w_d}$ と書くことにする。。$w_d = 0$ のときは $w_da_d = 0$ になることに注意すると、
劣勾配は以下のようになる。</p>
<p>$$
\frac{\partial J_1}{\partial w_d} =
\begin{cases}
\{ -c_d + w_da_d - \lambda \} &amp; (w_d &lt; 0)\\
[ -c_d-\lambda, -c_d + \lambda ] &amp; (w_d = 0)\\
\{ -c_d + w_da_d + \lambda \} &amp; (w_d &gt; 0)\\
\end{cases}
$$</p>
<p>&ldquo;勾配 = 0&rdquo; を解く代わりに &ldquo;0 $\in$ 劣勾配&rdquo; を満たす $\bm{w}$ を求める。やや天下り的ではあるが、$c_d$ の値で場合分けする。</p>
<ol>
<li>$c_d &lt; -\lambda$ のとき、$\hat{\bm{w}} = \frac{c_d + \lambda}{a_d}$ とおけば、$\hat{\bm{w}} &lt; 0$。
よって、上式の1行目に代入できて、$\frac{\partial J_1}{\partial w_d} = \{ 0 \}$。</li>
<li>$c_d &gt; \lambda$ のとき、$\hat{\bm{w}} = \frac{c_d - \lambda}{a_d}$ とおけば、$\hat{\bm{w}} &gt; 0$。
よって、上式の3行目に代入できて、$\frac{\partial J_1}{\partial w_d} = \{ 0 \}$。</li>
<li>$-\lambda \le c_d \le \lambda$ のとき、$-c_d-\lambda \le 0 \le -c_d+\lambda$ すなわち $0 \in [-c_d-\lambda, -c_d+\lambda]$。
そこで、$\hat{\bm{w}} = 0$ とすれば、上式の2行目より $0 \in \frac{\partial J_1}{\partial w_d}$。</li>
</ol>
<p>まとめると、&ldquo;0 $\in$ 劣勾配&rdquo; を満たす $\bm{w}$ は以下のようになる。</p>
<p>$$
\hat{\bm{w}} =
\begin{cases}
\frac{c_d + \lambda}{a_d} &amp; (c_d &lt; -\lambda)\\
0 &amp; (-\lambda \le c_d \le \lambda)\\
\frac{c_d - \lambda}{a_d} &amp; (c_d &gt; \lambda)\\
\end{cases}
$$</p>
<p>一般に、ソフト閾値作用素 (soft thresholding operator)というものがある。それは以下のように定義される。</p>
<p>$$
S(\theta, \lambda) =
\begin{cases}
\theta + \lambda &amp; (\theta &lt; -\lambda)\\
0 &amp; (-\lambda \le \theta \le \lambda)\\
\theta - \lambda &amp; (\theta &gt; \lambda)\\
\end{cases}
$$</p>
<p>これを用いて、$\hat{\bm{w}} = S(\frac{c_d}{a_d}, \frac{\lambda}{a_d})$ と書ける。</p>
<h3 id="座標降下法">座標降下法</h3>
<p>各ステップで以下の手順を行う。</p>
<ol>
<li>$d = 1, 2, \ldots, D$ について、$\bm{w} \leftarrow \argmin_{w_d} J_1(\bm{w})$</li>
</ol>
<p>つまり、色々な $d$ で、$w_d$ についての最小化問題を解くことを繰り返す。</p>
<p>$\argmin_{w_d} J_1(\bm{w})$ は「$w_d$ だけ動かした場合に、$J_1$ が最小となる $w_d$」のことである。これは前に求めた $\hat{\bm{w}}$ のことであったから、
手順は、</p>
<p>$d = 1, 2, \ldots, D$ について、</p>
<ol>
<li>$a_d = \| \bm{x}_{:, d} \|_2^2$ を計算。</li>
<li>$c_d = x_{:, d}^T \bm{r}_{-d}$ を計算。</li>
<li>$\bm{w} \leftarrow S(\frac{c_d}{a_d}, \frac{\lambda}{a_d})$。</li>
</ol>
<p>と読み替えられる。</p>
<h2 id="juliaによる実装">Juliaによる実装</h2>
<p>雛形を用意．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">using</span> <span class="n">Plots</span>
<span class="k">using</span> <span class="n">Random</span><span class="p">,</span> <span class="n">Distributions</span>
<span class="k">using</span> <span class="n">LinearAlgebra</span>
<span class="n">gr</span><span class="p">()</span>

<span class="k">function</span> <span class="n">main</span><span class="p">()</span>
  <span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">2021</span><span class="p">)</span>

  <span class="c"># ここに色々書く</span>
<span class="k">end</span>

<span class="n">main</span><span class="p">()</span>
</code></pre></div><h3 id="データの生成">データの生成</h3>
<p>以下のモデルを満たすデータ$(x, y)$を生成する。
$$
y = w_0 + w_1x + \cdots + w_dx^d + \varepsilon
$$
ただし，$\varepsilon \sim \mathcal{N}(0, \sigma^2)$．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">generate_data</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">N</span><span class="p">;</span> <span class="n">sigma</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
  <span class="n">dist</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
  <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">evalpoly</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">rand</span><span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
<span class="k">end</span>
</code></pre></div><h3 id="ridge回帰-1">Ridge回帰</h3>
<p>以下の勾配を使った勾配降下法を適用すればよい。</p>
<p>$$
\frac{\partial J_2}{\partial \bm{w}} = X^T(X\bm{w} - \bm{y}) + \lambda \bm{w}
$$</p>
<p>勾配はJuliaだと以下のように書ける。</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="n">X</span><span class="o">&#39;*</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">w</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span><span class="o">*</span><span class="n">w</span>
</code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">fit_by_ridge</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w0</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">lambda</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
  <span class="n">ws</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">]</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">max_iter</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">X</span><span class="o">&#39;*</span><span class="p">(</span><span class="n">X</span><span class="o">*</span><span class="n">w</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambda</span><span class="o">*</span><span class="n">w</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">dw</span>
    <span class="n">push!</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
  <span class="k">end</span>
  <span class="n">hcat</span><span class="p">(</span><span class="n">ws</span><span class="o">...</span><span class="p">)</span><span class="o">&#39;</span>
<span class="k">end</span>
</code></pre></div><h3 id="lasso回帰-1">Lasso回帰</h3>
<p>座標降下法による手順を再掲する。</p>
<p>$d = 1, 2, \ldots, D$ について、</p>
<ol>
<li>$a_d = \| \bm{x}_{:, d} \|_2^2$ を計算。</li>
<li>$c_d = x_{:, d}^T \bm{r}_{-d}$ を計算。</li>
<li>$\bm{w} \leftarrow S(\frac{c_d}{a_d}, \frac{\lambda}{a_d})$。</li>
</ol>
<p>$a_d$ については、単純に<code>X[:, d]</code>のノルムを計算すれば良い。$c_d$については、</p>
<p>$$
\begin{aligned}
x_{:, d}^T\bm{r}<em>{-d}
&amp;= \sum</em>{n=1}^{N} x_{nd}(y^{(n)} - \bm{w}<em>{-d}^T\bm{x}</em>{n,-d})\\
&amp;= \sum_{n=1}^{N} x_{nd}(y^{(n)} - \bm{w}^T\bm{x}<em>n + w_dx</em>{nd})\\
&amp;= \sum_{n=1}^{N} x_{nd}(y^{(n)} - \bm{w}^T\bm{x}<em>n) + w_d\sum</em>{n=1}^Nx_{nd}^2\\
&amp;= x_{:, d}^T(\bm{y} - X\bm{w}) + w_da_d
\end{aligned}
$$</p>
<p>と式変形すれば、</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span><span class="o">&#39;*</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">a</span>
</code></pre></div><p>と計算できる。</p>
<p>ソフト閾値作用素 $S$ は次のように定義できる．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">soft_thresholding</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">lambda</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">theta</span> <span class="o">&lt;</span> <span class="o">-</span><span class="n">lambda</span>
    <span class="n">theta</span> <span class="o">+</span> <span class="n">lambda</span>
  <span class="k">elseif</span> <span class="n">theta</span> <span class="o">&gt;</span> <span class="n">lambda</span>
    <span class="n">theta</span> <span class="o">-</span> <span class="n">lambda</span>
  <span class="k">else</span>
    <span class="mi">0</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div><p>これを用いてLasso回帰のアルゴリズムを定義．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">fit_by_lasso</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w0</span><span class="p">;</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">lambda</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
  <span class="n">D</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">w0</span><span class="p">)</span>
  <span class="n">ws</span> <span class="o">=</span> <span class="p">[</span><span class="n">w0</span><span class="p">]</span>
  <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span>
  <span class="k">for</span> <span class="n">_</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">max_iter</span>
    <span class="k">for</span> <span class="n">d</span> <span class="k">in</span> <span class="mi">1</span><span class="o">:</span><span class="n">D</span>
      <span class="n">a_d</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span>
      <span class="n">c_d</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="o">:</span><span class="p">,</span> <span class="n">d</span><span class="p">]</span><span class="o">&#39;</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">X</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">*</span> <span class="n">a_d</span>
      <span class="n">w</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">=</span> <span class="n">soft_thresholding</span><span class="p">(</span><span class="n">c_d</span><span class="o">/</span><span class="n">a_d</span><span class="p">,</span> <span class="n">lambda</span><span class="o">/</span><span class="n">a_d</span><span class="p">)</span>
      <span class="n">push!</span><span class="p">(</span><span class="n">ws</span><span class="p">,</span> <span class="n">copy</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
    <span class="k">end</span>
  <span class="k">end</span>
  <span class="n">hcat</span><span class="p">(</span><span class="n">ws</span><span class="o">...</span><span class="p">)</span><span class="o">&#39;</span>
<span class="k">end</span>
</code></pre></div><h3 id="グラフのプロット">グラフのプロット</h3>
<p>予測関数を描画する関数を定義．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">plot_w!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span>
  <span class="n">d</span> <span class="o">=</span> <span class="n">length</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">plot_x</span> <span class="o">=</span> <span class="n">range</span><span class="p">(</span><span class="n">extrema</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">...</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
  <span class="n">plot!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">plot_x</span><span class="p">,</span> <span class="n">x</span> <span class="o">-&gt;</span> <span class="n">w</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">poly_vec</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div><h3 id="main関数">main関数</h3>
<p>まずはRidge回帰、Lasso回帰の結果をグラフで図示してみる．
ついでに普通の最小二乗法で解いた$w$もプロットしてみる．</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-julia" data-lang="julia"><span class="k">function</span> <span class="n">poly_vec</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
  <span class="p">[</span><span class="n">x</span><span class="o">^</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="k">in</span> <span class="mi">0</span><span class="o">:</span><span class="n">d</span><span class="p">]</span>
<span class="k">end</span>

<span class="k">function</span> <span class="n">main</span><span class="p">()</span>
  <span class="n">Random</span><span class="o">.</span><span class="n">seed!</span><span class="p">(</span><span class="mi">2021</span><span class="p">)</span>
  <span class="n">p</span> <span class="o">=</span> <span class="n">plot</span><span class="p">()</span>

  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">generate_data</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">2.0</span><span class="p">,</span><span class="o">-</span><span class="mf">3.0</span><span class="p">],</span> <span class="mi">5</span><span class="p">)</span>

  <span class="n">d</span> <span class="o">=</span> <span class="mi">5</span>
  <span class="n">X</span> <span class="o">=</span> <span class="n">hcat</span><span class="p">(</span><span class="n">poly_vec</span><span class="o">.</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span><span class="o">...</span><span class="p">)</span><span class="o">&#39;</span>

  <span class="n">ws_ridge</span> <span class="o">=</span> <span class="n">fit_by_ridge</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">display</span><span class="p">(</span><span class="n">ws_ridge</span><span class="p">[</span><span class="k">end</span><span class="p">,</span> <span class="o">:</span><span class="p">])</span>

  <span class="n">ws_lasso</span> <span class="o">=</span> <span class="n">fit_by_lasso</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
  <span class="n">display</span><span class="p">(</span><span class="n">ws_lasso</span><span class="p">[</span><span class="k">end</span><span class="p">,</span> <span class="o">:</span><span class="p">])</span>

  <span class="n">w_overfitting</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span> <span class="o">\</span> <span class="p">(</span><span class="n">X</span><span class="o">&#39;</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span>

  <span class="n">p</span> <span class="o">=</span> <span class="n">plot</span><span class="p">()</span>
  <span class="n">scatter!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
  <span class="n">plot_w!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ws_ridge</span><span class="p">[</span><span class="k">end</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="s">&#34;ridge&#34;</span><span class="p">)</span>
  <span class="n">plot_w!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">ws_lasso</span><span class="p">[</span><span class="k">end</span><span class="p">,</span> <span class="o">:</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="s">&#34;lasso&#34;</span><span class="p">)</span>
  <span class="n">plot_w!</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">w_overfitting</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="s">&#34;overfitting&#34;</span><span class="p">)</span>
<span class="k">end</span>
</code></pre></div><p>最小二乗法の方は過学習を起こしているが、
Ridge回帰とLasso回帰は自然な曲線を描いていることが読み取れる。</p>
<figure><img src="img0.png"/>
</figure>

<p>$w$の値を見る。前者がRidge回帰、後者がLasso回帰。</p>
<pre class="cui">
6-element Vector{Float64}:
  1.185633350868915
  0.5514935176341534
 -0.3138792668283392
 -0.7019412321018753
 -0.5448897044829057
 -0.5005843434957847
6-element Vector{Float64}:
  1.2261293729701306
  0.27148775417222326
  0.0
 -0.45102159950359655
 -1.2632744627838859
  0.0
</pre>

<p>Lassoで特徴的なのは、一部のパラメータが完全に0になっている点。
0になったパラメータは、今回のモデルにとって必要のないパラメータであった、と解釈できる。
このように、Lasso回帰ではモデルにとって必要のないパラメータを見つけることができる。
ただし、「どの程度必要ないのか」は $\lambda$ の値によることに注意。$\lambda$ を大きくすればするほどパラメータが0になりやすくなる。</p>
<p>Lasso回帰ではいくつかのパラメータを捨てることになるが、すべてのパラメータを考慮したい場合にはRidge回帰を利用すれば良いのだと思う。</p>

</article>



</html>
